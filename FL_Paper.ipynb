{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlSXlBSA1wDmthlYbzb9q7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MustofAhmed41/3D-Jeep-Car/blob/main/FL_Paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z6O6bP1gQAqb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "yagNE8_AQG73"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST data\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Create a list to hold the data for each client\n",
        "clients_data = [[] for _ in range(10)]\n",
        "\n",
        "# Shuffle the dataset\n",
        "indices = np.random.permutation(len(train_images))\n",
        "shuffled_images = train_images[indices]\n",
        "shuffled_labels = train_labels[indices]\n",
        "\n",
        "# Partition the data equally among 30 clients\n",
        "for i in range(len(shuffled_images)):\n",
        "    label = shuffled_labels[i]\n",
        "    client_idx = i % 10  # Distribute evenly among 50 clients\n",
        "    clients_data[client_idx].append((shuffled_images[i], label))\n",
        "\n",
        "# Convert the list of lists to numpy arrays\n",
        "clients_data = [np.array(client_data) for client_data in clients_data]\n",
        "\n",
        "# Verify that all clients have an equal number of labels for all 10 digits\n",
        "for i, client_data in enumerate(clients_data):\n",
        "    labels, counts = np.unique(client_data[:, 1], return_counts=True)\n",
        "    print(f\"Client {i} label distribution: {dict(zip(labels, counts))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbfExsNDTFum",
        "outputId": "80d6f326-5313-4f04-c0e5-c6d34ff396ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-f34bc6f25370>:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  clients_data = [np.array(client_data) for client_data in clients_data]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 0 label distribution: {0: 610, 1: 664, 2: 611, 3: 594, 4: 564, 5: 536, 6: 591, 7: 621, 8: 597, 9: 612}\n",
            "Client 1 label distribution: {0: 612, 1: 684, 2: 583, 3: 601, 4: 635, 5: 503, 6: 584, 7: 646, 8: 578, 9: 574}\n",
            "Client 2 label distribution: {0: 612, 1: 727, 2: 551, 3: 638, 4: 573, 5: 509, 6: 606, 7: 603, 8: 584, 9: 597}\n",
            "Client 3 label distribution: {0: 596, 1: 700, 2: 606, 3: 602, 4: 578, 5: 562, 6: 597, 7: 605, 8: 586, 9: 568}\n",
            "Client 4 label distribution: {0: 570, 1: 648, 2: 577, 3: 627, 4: 572, 5: 546, 6: 602, 7: 665, 8: 605, 9: 588}\n",
            "Client 5 label distribution: {0: 582, 1: 666, 2: 586, 3: 621, 4: 588, 5: 537, 6: 555, 7: 646, 8: 577, 9: 642}\n",
            "Client 6 label distribution: {0: 572, 1: 653, 2: 581, 3: 607, 4: 591, 5: 602, 6: 588, 7: 627, 8: 616, 9: 563}\n",
            "Client 7 label distribution: {0: 573, 1: 643, 2: 640, 3: 639, 4: 591, 5: 527, 6: 588, 7: 621, 8: 560, 9: 618}\n",
            "Client 8 label distribution: {0: 620, 1: 675, 2: 609, 3: 603, 4: 558, 5: 534, 6: 602, 7: 603, 8: 582, 9: 614}\n",
            "Client 9 label distribution: {0: 576, 1: 682, 2: 614, 3: 599, 4: 592, 5: 565, 6: 605, 7: 628, 8: 566, 9: 573}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "ZCwAP2LKVw4J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers_shape = []"
      ],
      "metadata": {
        "id": "DAl_Sf2vk9Mx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j,client_data in enumerate(clients_data):\n",
        "  # Split the data into features (images) and labels\n",
        "  client_images, client_labels = list(client_data[:, 0]), list(client_data[:, 1])\n",
        "  # Normalize the image data\n",
        "  client_images = np.array(client_images) / 255.0\n",
        "\n",
        "  # Convert the labels to one-hot encoding\n",
        "  client_labels = tf.keras.utils.to_categorical(client_labels, 10)\n",
        "\n",
        "  # Create a TensorFlow Dataset from the NumPy arrays\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((client_images, client_labels)).shuffle(len(client_images)).batch(32)\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "  model.add(layers.Dense(256,  activation='relu', name=\"layer_1\"))\n",
        "  model.add(layers.Dense(512, activation='relu', name=\"layer_2\"))\n",
        "  model.add(layers.Dense(10, activation='softmax', name=\"layer_3\"))\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Train the model using the TensorFlow Dataset\n",
        "  model.fit(dataset, epochs=2)\n",
        "\n",
        "  for layer in model.layers:\n",
        "    if hasattr(layer, 'weights'):\n",
        "      weights = layer.get_weights()\n",
        "      for i, weight in enumerate(weights):\n",
        "        file_name = \"model_\" + str(j) + \"_\" + str(layer.name)\n",
        "        if i == 0:\n",
        "          file_name = file_name + \"_weights.csv\"\n",
        "          # print(file_name)\n",
        "          np.savetxt(file_name, weight, delimiter=',')\n",
        "        else:\n",
        "          file_name = file_name + \"_biases.csv\"\n",
        "          # print(file_name)\n",
        "          np.savetxt(file_name, weight, delimiter=',')\n",
        "\n",
        "  if j == len(clients_data)-1:\n",
        "    for layer in model.layers:\n",
        "      if hasattr(layer, 'weights'):\n",
        "        weights = layer.get_weights()\n",
        "        temp = []\n",
        "        for i, weight in enumerate(weights):\n",
        "          temp.append(weight.shape)\n",
        "\n",
        "        layers_shape.append(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgysLXF1Y6-3",
        "outputId": "f7e5333e-0255-4eb2-ecba-9a2b6cd54862"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "188/188 [==============================] - 6s 14ms/step - loss: 0.5150 - accuracy: 0.8492\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.2013 - accuracy: 0.9373\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 2s 7ms/step - loss: 0.5204 - accuracy: 0.8458\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 2s 11ms/step - loss: 0.2055 - accuracy: 0.9380\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.4884 - accuracy: 0.8563\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 0.1778 - accuracy: 0.9478\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 2s 7ms/step - loss: 0.5395 - accuracy: 0.8415\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 2s 11ms/step - loss: 0.2170 - accuracy: 0.9337\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 2s 7ms/step - loss: 0.5171 - accuracy: 0.8467\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 0.2045 - accuracy: 0.9363\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.5126 - accuracy: 0.8443\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.2063 - accuracy: 0.9375\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 3s 11ms/step - loss: 0.5294 - accuracy: 0.8407\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 0.2026 - accuracy: 0.9327\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 2s 7ms/step - loss: 0.5300 - accuracy: 0.8433\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 1s 8ms/step - loss: 0.2074 - accuracy: 0.9393\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 3s 11ms/step - loss: 0.5219 - accuracy: 0.8443\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 2s 9ms/step - loss: 0.2007 - accuracy: 0.9400\n",
            "Epoch 1/2\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.5238 - accuracy: 0.8437\n",
            "Epoch 2/2\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 0.2091 - accuracy: 0.9407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_weights(layer_name):\n",
        "  weight_file_name = layer_name + \"_weights.csv\"\n",
        "  bias_file_name = layer_name + \"_biases.csv\"\n",
        "  weight = my_array = np.loadtxt(weight_file_name, delimiter=',')\n",
        "  biases = my_array = np.loadtxt(bias_file_name, delimiter=',')\n",
        "  return [weight, biases]"
      ],
      "metadata": {
        "id": "WunphT65Y_NC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fed_avg():\n",
        "  for i,layer_name in enumerate(layer_names_lst):\n",
        "    temp = np.zeros((layers_shape[i][0][0], layers_shape[i][0][1]))\n",
        "    for j, model_name in enumerate(model_names_lst):\n",
        "      file_name  = model_name + layer_name + weights_biases[0]\n",
        "      temp = temp + np.loadtxt(file_name, dtype=np.float64, delimiter=\",\")\n",
        "    temp = temp / len(clients_data)\n",
        "    save_file_name = 'aggregated_' + layer_name + weights_biases[0]\n",
        "    np.savetxt(save_file_name, temp, delimiter=',')\n",
        "\n",
        "  for i,layer_name in enumerate(layer_names_lst):\n",
        "    temp = np.zeros((layers_shape[i][1][0]))\n",
        "    for j, model_name in enumerate(model_names_lst):\n",
        "      file_name  = model_name + layer_name + weights_biases[1]\n",
        "      temp = temp + np.loadtxt(file_name, dtype=np.float64, delimiter=\",\")\n",
        "    temp = temp / len(clients_data)\n",
        "    save_file_name = 'aggregated_' + layer_name + weights_biases[1]\n",
        "    np.savetxt(save_file_name, temp, delimiter=',')"
      ],
      "metadata": {
        "id": "nuEQf7awlQv2"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdGWOyXKlal7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregated_model():\n",
        "  latest_model = keras.models.Sequential()\n",
        "  latest_model.add(keras.layers.Dense(50, weights=get_model_weights(\"aggregated_layer_1\"),  activation='relu',  input_shape=(784,), name=\"layer_1\"))\n",
        "  latest_model.add(keras.layers.Dense(20, weights=get_model_weights(\"aggregated_layer_2\"),  activation='relu',  name=\"layer_2\"))\n",
        "  latest_model.add(keras.layers.Dense(num_classes, weights=get_model_weights(\"aggregated_layer_3\"), activation='softmax',  name=\"layer_3\"))\n",
        "\n",
        "  loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "  optim = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "  latest_model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
        "\n",
        "  print(\"Evaluate:\")\n",
        "  x_train, y_train, x_test, y_test = create_dataset()\n",
        "\n",
        "  latest_model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "LQEGPm6LloYS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_names_lst = [\"layer_1_\", \"layer_2_\", \"layer_3_\"]\n",
        "model_names_lst = []\n",
        "weights_biases = [\"weights.csv\", \"biases.csv\"]\n",
        "\n",
        "for i in range(1, len(clients_data)+1):\n",
        "  model = \"model_\" + str(i) + \"_\"\n",
        "  model_names_lst.append(model)\n",
        "\n",
        "print(layer_names_lst)\n",
        "print(model_names_lst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqIRapFir5UV",
        "outputId": "7a317fcc-ff5a-4815-80aa-5691eaf33563"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['layer_1_', 'layer_2_', 'layer_3_']\n",
            "['model_1_', 'model_2_', 'model_3_', 'model_4_', 'model_5_', 'model_6_', 'model_7_', 'model_8_', 'model_9_', 'model_10_']\n",
            "layer_1_\n",
            "layer_2_\n",
            "layer_3_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fed_avg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "JHFGQH-6l3MZ",
        "outputId": "3b5342cf-293a-473b-ad77-e8dc149cb732"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-8ba4ae85e79c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfed_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-ac1cd032360e>\u001b[0m in \u001b[0;36mfed_avg\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfed_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mfile_name\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweights_biases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8T7zPZql6AY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}